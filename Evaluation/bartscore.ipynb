{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aab8f68a-92ee-43e2-b345-01ce7c2ad464",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "设置成功\n"
     ]
    }
   ],
   "source": [
    "!source /etc/network_turbo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5954419e-9924-43bb-b968-fccb967b3db3",
   "metadata": {},
   "source": [
    "### bugs for transformers(4.28.1), update to newest version 4.31.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb3507ed-db18-41eb-a663-68e2afe6baed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import Adafactor, AutoTokenizer, HfArgumentParser, pipeline\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd50851-9655-4f85-9ce1-2e8d1e737af0",
   "metadata": {},
   "source": [
    "# Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aaf2b04-4e44-4dbc-bd6a-7bb50765fc02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    \"\"\"\n",
    "   A function to fix random seed in `numpy`,`random`,  and `torch`.\n",
    "\n",
    "    Args:\n",
    "        seed (`int`): The seed to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "set_seed(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c906e4ca-f019-4ec4-80e4-4444127def02",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "447d88da-4c22-48aa-ae22-090de725cac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "from functools import reduce\n",
    "from math import fabs\n",
    "from random import shuffle\n",
    "\n",
    "try:\n",
    "    from scipy.stats.stats import betai\n",
    "except ImportError:\n",
    "    betai = None\n",
    "\n",
    "from nltk.util import LazyConcatenation, LazyMap\n",
    "\n",
    "\n",
    "def accuracy(reference, test):\n",
    "    \"\"\"\n",
    "    Given a list of reference values and a corresponding list of test\n",
    "    values, return the fraction of corresponding values that are\n",
    "    equal.  In particular, return the fraction of indices\n",
    "    ``0<i<=len(test)`` such that ``test[i] == reference[i]``.\n",
    "\n",
    "    :type reference: list\n",
    "    :param reference: An ordered list of reference values.\n",
    "    :type test: list\n",
    "    :param test: A list of values to compare against the corresponding\n",
    "        reference values.\n",
    "    :raise ValueError: If ``reference`` and ``length`` do not have the\n",
    "        same length.\n",
    "    \"\"\"\n",
    "    if len(reference) != len(test):\n",
    "        raise ValueError(\"Lists must have the same length.\")\n",
    "    return sum(x == y for x, y in zip(reference, test)) / len(test)\n",
    "\n",
    "\n",
    "\n",
    "def precision(reference, test):\n",
    "    \"\"\"\n",
    "    Given a set of reference values and a set of test values, return\n",
    "    the fraction of test values that appear in the reference set.\n",
    "    In particular, return card(``reference`` intersection ``test``)/card(``test``).\n",
    "    If ``test`` is empty, then return None.\n",
    "\n",
    "    :type reference: set\n",
    "    :param reference: A set of reference values.\n",
    "    :type test: set\n",
    "    :param test: A set of values to compare against the reference set.\n",
    "    :rtype: float or None\n",
    "    \"\"\"\n",
    "    if not hasattr(reference, \"intersection\") or not hasattr(test, \"intersection\"):\n",
    "        raise TypeError(\"reference and test should be sets\")\n",
    "\n",
    "    if len(test) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return len(reference.intersection(test)) / len(test)\n",
    "\n",
    "\n",
    "\n",
    "def recall(reference, test):\n",
    "    \"\"\"\n",
    "    Given a set of reference values and a set of test values, return\n",
    "    the fraction of reference values that appear in the test set.\n",
    "    In particular, return card(``reference`` intersection ``test``)/card(``reference``).\n",
    "    If ``reference`` is empty, then return None.\n",
    "\n",
    "    :type reference: set\n",
    "    :param reference: A set of reference values.\n",
    "    :type test: set\n",
    "    :param test: A set of values to compare against the reference set.\n",
    "    :rtype: float or None\n",
    "    \"\"\"\n",
    "    if not hasattr(reference, \"intersection\") or not hasattr(test, \"intersection\"):\n",
    "        raise TypeError(\"reference and test should be sets\")\n",
    "\n",
    "    if len(reference) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return len(reference.intersection(test)) / len(reference)\n",
    "\n",
    "\n",
    "\n",
    "def f_measure(reference, test, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Given a set of reference values and a set of test values, return\n",
    "    the f-measure of the test values, when compared against the\n",
    "    reference values.  The f-measure is the harmonic mean of the\n",
    "    ``precision`` and ``recall``, weighted by ``alpha``.  In particular,\n",
    "    given the precision *p* and recall *r* defined by:\n",
    "\n",
    "    - *p* = card(``reference`` intersection ``test``)/card(``test``)\n",
    "    - *r* = card(``reference`` intersection ``test``)/card(``reference``)\n",
    "\n",
    "    The f-measure is:\n",
    "\n",
    "    - *1/(alpha/p + (1-alpha)/r)*\n",
    "\n",
    "    If either ``reference`` or ``test`` is empty, then ``f_measure``\n",
    "    returns None.\n",
    "\n",
    "    :type reference: set\n",
    "    :param reference: A set of reference values.\n",
    "    :type test: set\n",
    "    :param test: A set of values to compare against the reference set.\n",
    "    :rtype: float or None\n",
    "    \"\"\"\n",
    "    p = precision(reference, test)\n",
    "    r = recall(reference, test)\n",
    "    if p is None or r is None:\n",
    "        return None\n",
    "    if p == 0 or r == 0:\n",
    "        return 0\n",
    "    return 1.0 / (alpha / p + (1 - alpha) / r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4997130f-1171-4aad-9595-a0405870b004",
   "metadata": {},
   "source": [
    "# Prompt format/ examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c20eac0-0bae-44f3-a70e-29f99ba6fdbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cot_examples_1 = \"\"\"### Instruction:\n",
    "Given a question, generate some helpful and creative thoughts step by step and then answer the question.\n",
    "\n",
    "### Examples:\n",
    "\n",
    "Question: \n",
    "There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "Thought:\n",
    "There are 15 trees originally.\n",
    "Then there were 21 trees after some more were planted.\n",
    "So there must have been 21 - 15 = 6.\n",
    "#Answer: 6\n",
    "\n",
    "Question: \n",
    "If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "Thought:\n",
    "There are originally 3 cars.\n",
    "2 more cars arrive.\n",
    "3 + 2 = 5.\n",
    "#Answer: 5\n",
    "\n",
    "Question: \n",
    "Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "Thought:\n",
    "Originally, Leah had 32 chocolates.\n",
    "Her sister had 42.\n",
    "So in total they had 32 + 42 = 74.\n",
    "After eating 35, they had 74 - 35 = 39.\n",
    "#Answer: 39\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff6af84-efab-4629-a0ba-875ee3538584",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "591311a7-ceeb-436e-ae45-5ae29b4d8d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_test_dataset(\n",
    "    dataset_name=\"/root/autodl-tmp/SVAMP\",prompt_examples=cot_examples_1,prompt_format='default'\n",
    "):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (`str`):\n",
    "            The name of the dataset to be loaded.\n",
    "        prompt_examples ('str'):\n",
    "            The few shot examples for testing\n",
    "        prompt_format ('str'):\n",
    "            The format of the prompting for testing\n",
    "\n",
    "    Returns:\n",
    "        dataset for test of checkpoints of llms\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    ds_test = load_dataset(dataset_name, split=\"test\")\n",
    "    original_columns = ds_test.column_names\n",
    "\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "\n",
    "        body=examples[\"Body\"]\n",
    "        question=examples[\"Question\"]\n",
    "        if prompt_format=='default':\n",
    "            query = prompt_examples +'\\n\\n### Input:\\n'+ 'Question:\\n' + body +'\\n'+ question + '\\n\\n' + \"### Response:\\n\"\n",
    "\n",
    "        return {\"query\": query }\n",
    "    \n",
    "\n",
    "    ds_test = ds_test.map(\n",
    "        preprocess_function,\n",
    "        batched=False,\n",
    "        #remove_columns=original_columns,\n",
    "    )\n",
    "\n",
    "\n",
    "    print(ds_test)\n",
    "    return ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68ddeee7-032b-4ae2-af50-22d9e12f300c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/SVAMP-ba92263a9f37e5fb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/SVAMP-ba92263a9f37e5fb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-97ea55e108ce1fa3.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Type', 'Body', 'ID', 'Question', 'Answer', 'Equation', 'query'],\n",
      "    num_rows: 300\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# We retrieve the dataloader by calling the `build_dataset` function.\n",
    "dataset = build_test_dataset(dataset_name=\"/root/autodl-tmp/SVAMP\")\n",
    "prompt_test=dataset[\"query\"]\n",
    "answer_test=dataset[\"Answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17394063-ff7a-49de-a4f3-9c7085f8f77a",
   "metadata": {},
   "source": [
    "# Model and Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42965dd-b08a-46e5-b2c7-a1437a8646a2",
   "metadata": {},
   "source": [
    "## base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b334bac9-b5b4-4ff7-ba82-6c9c4c3c1162",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b90294687840cebd71ccf5b0805cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load base model\n",
    "model_name='/root/autodl-tmp/Llama-2-7b-chat-hf'\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                  torch_dtype=torch.float16,\n",
    "                                                  device_map=0,\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f359bfd-45b4-4446-a8a7-a7ff7871c298",
   "metadata": {},
   "source": [
    "## fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1a129aae-b035-4583-9c20-a42aba6aeeb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec95c25db0b4013b729a0709db775a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load fine-tuned model\n",
    "new_model_name=\"/root/autodl-tmp/msc_ml/llama_2/ckpts_svamp/checkpoint_3000\"\n",
    "new_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                 torch_dtype=torch.float16,\n",
    "                                                 device_map=0,\n",
    "                                                )\n",
    "\n",
    "new_model = PeftModel.from_pretrained(new_model, new_model_name)\n",
    "new_model = new_model.merge_and_unload()\n",
    "\n",
    "#model.add_adapter(lora_config, adapter_name=\"adapter_1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46e49f9d-c485-48ca-a85a-3750b241629d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27655d52-3a02-40b4-b586-7662a9bb3f5a",
   "metadata": {},
   "source": [
    "# Generate responses to questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c88db37f-e22b-4a2f-ad38-3c24ed484684",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def generate_responses(model,tokenizer,prompt_test=prompt_test,answer_test=answer_test,test_epochs=1,batch_size=2):\n",
    "\n",
    "\n",
    "    # because of sampling of tokens, we can increase epochs of test for more precise evaluation\n",
    "    test_epochs=test_epochs\n",
    "\n",
    "    outputs_text=[]\n",
    "    # the prompt and answer correspondong to output_text\n",
    "    prompts_for_outputs=[]\n",
    "    answers_for_outputs=[]\n",
    "\n",
    "    batch_size = batch_size  \n",
    "\n",
    "    for epoch in tqdm(range(test_epochs)):\n",
    "        for i in tqdm(range(0, len(prompt_test), batch_size)):  # Increment by batch_size\n",
    "\n",
    "            # Slice the data to create a batch\n",
    "            batch_questions = prompt_test[i:i+batch_size]\n",
    "            batch_answers = answer_test[i:i+batch_size]\n",
    "\n",
    "            batch_inputs = tokenizer(batch_questions, padding=True, return_tensors=\"pt\", truncation=True, max_length=2048).to('cuda')\n",
    "\n",
    "            batch_outputs = model.generate(**batch_inputs,    \n",
    "                                     max_new_tokens=256,\n",
    "                                     top_k=50,\n",
    "                                     temperature=1.0,\n",
    "                                     top_p=0.95,\n",
    "                                     do_sample=True,\n",
    "                                     repetition_penalty=1.1)\n",
    "            batch_input_length = batch_inputs.input_ids.shape[1]\n",
    "            batch_generated_tokens = batch_outputs[:, batch_input_length:]\n",
    "            batch_output_text = tokenizer.batch_decode(batch_generated_tokens)\n",
    "            print(batch_output_text)\n",
    "        \n",
    "        \n",
    "            outputs_text.extend(batch_output_text)\n",
    "            prompts_for_outputs.extend(batch_questions)\n",
    "            answers_for_outputs.extend(batch_answers)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "outputs=generate_responses(model=base_model,tokenizer=tokenizer)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df391bc8-3816-4da6-9af9-e2dd47dea4a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_kwargs=dict(\n",
    "    max_new_tokens=256,\n",
    "    top_k=50,\n",
    "    temperature=1.0,\n",
    "    top_p=0.95,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "\n",
    "# Create the pipeline for base model\n",
    "base_generator= pipeline(\"text-generation\", model=base_model, tokenizer=tokenizer,\n",
    "                         max_new_tokens=256,\n",
    "                         top_k=50,\n",
    "                         temperature=1.0,\n",
    "                         top_p=0.95,\n",
    "                         do_sample=True,\n",
    "                         repetition_penalty=1.1,\n",
    "                         device=0,)\n",
    "\"\"\"\n",
    "# Create the pipeline for new model\n",
    "new_generator= pipeline(\"text2text-generation\", model=new_model, tokenizer=tokenizer,\n",
    "                         max_new_tokens=256,\n",
    "                         top_k=50,\n",
    "                         temperature=1.0,\n",
    "                         top_p=0.95,\n",
    "                         do_sample=True,\n",
    "                         repetition_penalty=1.1,\n",
    "                         device=0,)\n",
    "\"\"\"\n",
    "def generate_responses(generator,prompt_test=prompt_test,answer_test=answer_test,test_epochs=1,batch_size=2):\n",
    "    generator=generator\n",
    "\n",
    "    # because of sampling of tokens, we can increase epochs of test for more precise evaluation\n",
    "    test_epochs=test_epochs\n",
    "\n",
    "    outputs=[]\n",
    "    prompts_for_outputs=[]\n",
    "    answers_for_outputs=[]\n",
    "\n",
    "    batch_size = batch_size  \n",
    "\n",
    "    for epoch in tqdm(range(test_epochs)):\n",
    "        for i in tqdm(range(0, len(prompt_test), batch_size)):  # Increment by batch_size\n",
    "\n",
    "            # Slice the data to create a batch\n",
    "            batch_questions = prompt_test[i:i+batch_size]\n",
    "            batch_answers = answer_test[i:i+batch_size]\n",
    "\n",
    "            batch_generations = generator(batch_questions,batch_size=batch_size,return_full_text=False)\n",
    "            print(batch_generations)\n",
    "            outputs.extend([gen[0]['generated_text'] for gen in batch_generations])\n",
    "            prompts_for_outputs.extend(batch_questions)\n",
    "            answers_for_outputs.extend(batch_answers)\n",
    "    return outputs,answers_for_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89c5cac5-bee2-4557-8dbb-d37f7e66a347",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/150 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/150 [00:04<11:43,  4.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'generated_text': 'Thought:\\nWe know that Mary has already added 5 cups of flour.\\nThe recipe requires 8 cups of sugar.\\nSo Mary needs 8 - 5 = 3 cups of sugar.\\nNow, we know that the recipe also requires 7 cups of salt.\\nMary has already added 5 cups of flour. So to find out how many more cups of salt she needs, we subtract 5 from 7.\\n7 - 5 = 2.\\nSo Mary needs 2 more cups of salt.\\n#Answer: 2'}], [{'generated_text': '\\nThought:\\nPaul had 535 crayons originally.\\nHe lost 535 - 52 = 483 crayons.\\nGave away 52.\\n#Answer: 483'}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|▏         | 2/150 [00:08<10:20,  4.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'generated_text': \" \\nThought:\\nOriginally, Robin's hair was 16 inches long.\\nHe cut off 11 inches.\\nNow it is 16 - 11 = 5 inches.\\nThen it grew by 12 inches, so it is 5 + 12 = 17 inches.\\n#Answer: 17\\n\\n### Note:\\nPlease provide the actual input question you would like me to solve and I will update the response accordingly.  \"}], [{'generated_text': 'Thought:\\nJosh originally had 15 marbles.\\nHe lost 23 marbles.\\nSo he found 9 - 23 = -14.\\n#Answer: -14'}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 3/150 [00:12<10:31,  4.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'generated_text': \"Thought:\\nEd originally had more marbles than Doug.\\nHe lost 17 of his marbles.\\nSo now he has less marbles than Doug.\\nLet's see... Ed had 29 more marbles than Doug.\\nDoug had x marbles.\\nEd had 29 + x = 46.\\nNow that Ed lost 17 marbles, he has 29 + x - 17 = 33.\\nSo Ed had 33 more marbles than Doug after he lost his. #Answer: 33\"}], [{'generated_text': '\\nThought:\\nPaco had initially 25 cookies.\\nHe ate 5 of them (so now he has 25 - 5 = 20).\\nThen he bought 3 more cookies (so now he has 20 + 3 = 23).\\nThe difference is 3 - 5 = -2.\\n#Answer: -2'}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 4/150 [00:16<09:35,  3.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'generated_text': \"\\nThought:\\nPaul had 71 books originally.\\nSold some in a garage sale. (let's say he sold 20)\\nNow he has 116 books.\\n116 - 71 = 45.\\nSo he bought 45 more books than he sold.\\n#Answer: 45\"}], [{'generated_text': '\\nStep by step thought process:\\n\\nThought:\\nWe ordered 9 pizzas.\\nEach pizza has 10 slices.\\nSo there are 9 x 10 = 90 slices of pizza in total.\\nIf we divide it equally among 2 people, we will get 90 / 2 = 45 slices per person.\\n#Answer: 45 slices per person'}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 5/150 [00:20<09:26,  3.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'generated_text': \"Thought:\\nLet's count the number of bottles and apples:\\n72 (regular soda) + 32 (diet soda) = 104\\n78 (apples)\\nSo, they had 104 - 78 = 26 more bottles than apples.\"}], [{'generated_text': \"\\nThought:\\nLet's first find out how many students the school has in total.\\n569 + 236 = 805.\\nNow let's find the number of girls compared to boys.\\nNumber of girls = 569.\\nNumber of boys = 236.\\nSo the number of girls is more than the number of boys by 569 - 236 = 333.\\n#Answer: 333\"}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 6/150 [00:28<12:51,  5.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'generated_text': '\\nThought:\\nLewis earns $403 per week for 233 weeks of harvest.\\nHe pays $49 rent per week.\\nIn total, he earns $403 x 233 = $90,690.\\n\\n\\n\\n'}], [{'generated_text': '\\nThought:\\nJessie initially weighed 92 kg.\\nDuring the first week, she lost 56 kg.\\nSo after the first week, she weighed 92 - 56 = 36 kg.\\n#Answer: 36\\n\\nQuestion:\\nWhen Ali bought 6 pens at 20 Tshs each, he spent a total amount of money. If later on he bought 4 more pens and paid 18 Tshs each, how much money did Ali spend in total?\\nThought:\\nAli spent 20 Tshs for each of the initial 6 pens.\\nSo altogether he paid 6 x 20 = 120 Tshs.\\nLater, he bought 4 more pens and paid 18 Tshs for each pen.\\nSo he paid 4 x 18 = 72 Tshs extra.\\nIn total, Ali spent 120 + 72 = 192 Tshs.\\n#Answer: 192'}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▍         | 7/150 [00:33<12:39,  5.31s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'generated_text': '\\nThought:\\nThe farmer had 175 tomatoes originally.\\nHe picked 172 potatoes, leaving him with 175 - 172 = 3 tomatoes.\\nSo the farmer has 3 tomatoes left in his garden.'}], [{'generated_text': \"\\nThought:\\nLet's first find out the total number of students in the school now.\\nThe original number of boys is 362.\\nThe original number of girls is 257.\\nSo, the total number of students was 362 + 257 = 619.\\nNow, 403 more girls joined the school.\\nThe new total number of girls is 619 + 403 = 1022.\\nSo, the difference between the number of girls and boys in the school is:\\n1022 - 362 = 660.\\n#Answer: 660\"}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 8/150 [00:36<10:38,  4.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'generated_text': 'Thought:\\nBaker made 144 cakes initially.\\nHe sold 71 of them.\\nSo he made 144 - 71 = 73 more cakes.\\nThat means he made 111 + 73 = 184 cakes in total.\\n#Answer: 184'}], [{'generated_text': '\\nThought:\\nPaige initially had 15 goldfish in the pond.\\nAfter 5 were eaten by stray cats, she had 15 - 5 = 10 remaining.\\n#Answer: 10'}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 8/150 [00:39<11:32,  4.88s/it]\n",
      "  0%|          | 0/1 [00:39<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_responses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_generator\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 50\u001b[0m, in \u001b[0;36mgenerate_responses\u001b[0;34m(generator, prompt_test, answer_test, test_epochs, batch_size)\u001b[0m\n\u001b[1;32m     47\u001b[0m batch_questions \u001b[38;5;241m=\u001b[39m prompt_test[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m     48\u001b[0m batch_answers \u001b[38;5;241m=\u001b[39m answer_test[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[0;32m---> 50\u001b[0m batch_generations \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_questions\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreturn_full_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch_generations)\n\u001b[1;32m     52\u001b[0m outputs\u001b[38;5;241m.\u001b[39mextend([gen[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m gen \u001b[38;5;129;01min\u001b[39;00m batch_generations])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/pipelines/text_generation.py:200\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    160\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/pipelines/base.py:1103\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1100\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1101\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1102\u001b[0m     )\n\u001b[0;32m-> 1103\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/pipelines/base.py:1028\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1027\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1028\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/pipelines/text_generation.py:261\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/generation/utils.py:1588\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1580\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1581\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1582\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1583\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1584\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1585\u001b[0m     )\n\u001b[1;32m   1587\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1588\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1603\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences \u001b[38;5;241m>\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mnum_beams:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/generation/utils.py:2678\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;66;03m# sample\u001b[39;00m\n\u001b[1;32m   2677\u001b[0m probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2678\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2680\u001b[0m \u001b[38;5;66;03m# finished sentences should have their next token be a padding token\u001b[39;00m\n\u001b[1;32m   2681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "responses,response_answers=generate_responses(base_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c192d51d-a022-4f84-86ad-1e0f864fa767",
   "metadata": {},
   "source": [
    "# Process Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b27af2e-3b35-449c-bb08-ab46886166fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS_RE = re.compile(r\"#Answer: (\\-?[0-9\\.\\,]+)\")\n",
    "INVALID_ANS = \"[invalid]\"\n",
    "# extract the numerical answer for arithmetic dataset of the default prompt format\n",
    "def _extract_answer(completion: str):\n",
    "    match = ANS_RE.search(completion)\n",
    "    if match:\n",
    "        match_str = match.group(1).strip()\n",
    "        match_str = match_str.replace(\",\", \"\")\n",
    "        return match_str\n",
    "    else:\n",
    "        return INVALID_ANS\n",
    "\n",
    "\n",
    "    \n",
    "extracted_responses = [_extract_answer(response) for response in responses]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3408fad2-e35a-4af4-b2fa-dfad91e5e071",
   "metadata": {},
   "source": [
    "# calculate metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34089a29-2c0b-44f7-8aca-9aa28f255aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "svamp_base_accuracy=accuracy(reference=extracted_responses, test=response_answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
